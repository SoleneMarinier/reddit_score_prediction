{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOWhut7cHE2h"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict the most accurately the score of a comment given:\n",
    "* Its usual features (day, time... See Notebook1)\n",
    "* Its network features (score of the parent comment... See Notebook 01)\n",
    "* Its textual content: see text mining in notebook 21 (sentiment analysis), 2 (preprocessing), 22 (word association), 3 (TF IDF).\n",
    "\n",
    "We decided on using a LIGHT GBM regression model as it is known to be one of the most accurate for Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the lightgbm library is not installed:\n",
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input your repository path here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repsource = \"/Users/alicetourret/Downloads/au_secours/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=open(repsource+\"df_body_cleaned\",\"rb\")\n",
    "df_cleaned=pickle.load(file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop([\"author\",\"link_id\",\"name\",\"parent_id\",\"popularity\",\"top_comment\",\"active_user2\",\"active_user3\",\n",
    "                            \"active_user4\",\"parent_seg1\",\"parent_seg2\",\"parent_seg3\",\"parent_seg4\",\"monday\",\"tuesday\",\n",
    "                            \"thursday\",\"friday\", \"saturday\",\"sunday\",\"wednesday\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cleaned.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.day_week = df_cleaned.day_week.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(dataset):\n",
    "    Test_DF = dataset[pd.isna(dataset.ups)]\n",
    "    return Test_DF\n",
    "\n",
    "def train_split(dataset):\n",
    "    Train_DF = dataset[pd.isna(dataset.ups) == False]\n",
    "    return Train_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_DF = train_split(df_cleaned)\n",
    "Test_DF = test_split(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_DF.shape)\n",
    "print(Test_DF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = Test_DF.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = Train_DF['ups']\n",
    "train_feats = Train_DF[Train_DF.columns.difference(['ups', 'body', 'id'])]\n",
    "test_target = Test_DF['ups']\n",
    "test_feats = Test_DF[Test_DF.columns.difference(['ups', 'body', 'id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_feats.shape)\n",
    "print(test_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Dense dataframe Sparse, and Combine with TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats_mat = csr_matrix(train_feats.values)\n",
    "test_feats_mat = csr_matrix(test_feats.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2=open(repsource+\"word_features\",\"rb\")\n",
    "train_word_features=pickle.load(file2)\n",
    "test_word_features=pickle.load(file2)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([train_feats_mat,train_word_features])\n",
    "testing = hstack([test_feats_mat,test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X,train_target,test_size=0.33, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': ['l2', 'auc'],\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 0,\n",
    "    \"max_depth\": 35,\n",
    "    \"num_leaves\": 500,  \n",
    "    \"num_iterations\": 1000,\n",
    "    \"n_estimators\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.LGBMRegressor(**hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8ELyMpjhY9v"
   },
   "source": [
    "**SUBMISSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = gbm.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test_id,'predicted': test_pred}).astype(dtype={'id': 'string','predicted': 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(repsource+\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**\n",
    "* We trained also the model removing the variables relative the sentiment (positive, neutral, negative comment). It doesn't deteriorate the performance of the model. These scores are interesting for the textual analysis but not very significant for the current model.\n",
    "* We could also tune the model trying to opimize the hyperparameters but we think the current model could be improved in terms of features especially. This one results in a private score of 14.75235 and a public score of 14.90260."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
