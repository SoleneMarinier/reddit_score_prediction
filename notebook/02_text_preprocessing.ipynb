{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsCM12aRN4NH"
   },
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is dedicated to text preprocessing, such as removing punctuation, tokenizing the sentences, etc. This step is necessary before running TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s1027177\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s1027177\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input your repository path here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repsource = \"C:/Users/s1027177/OneDrive - Syngenta/Documents/FOAD/au_secours/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(repsource+\"new_df\",\"rb\")\n",
    "new_df=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ups', 'link_id', 'name', 'id', 'author', 'body', 'parent_id',\n",
       "       'popularity', 'day_week', 'monday', 'tuesday', 'wednesday', 'thursday',\n",
       "       'friday', 'saturday', 'sunday', 'hour', 'ups_mean', 'active_user1',\n",
       "       'active_user2', 'active_user3', 'active_user4', 'top_comment',\n",
       "       'rank_comment', 'parent_ups', 'seconds_after_parent', 'parent_seg1',\n",
       "       'parent_seg2', 'parent_seg3', 'parent_seg4', 'parent_ups_mean',\n",
       "       'positive_com', 'neutral_com', 'negative_com'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will focus on the analysis of textual data.\n",
    "On `body`:\n",
    "* Cleaning: lower-casing, punctuation removal, stop words removal, lemmatization\n",
    "* Tokenization: cut comments into separate words\n",
    "* Modelling: a simple TF-IDF, Word2vec is the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xAIIlpiQj0g-"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_lowercase(df, colname):\n",
    "    df[colname] = df[colname].str.lower()\n",
    "    return df\n",
    "    \n",
    "def not_regex(pattern):\n",
    "    return r\"((?!{}).)\".format(pattern)\n",
    "\n",
    "def remove_punctuation(df, colname):\n",
    "    df[colname] = df[colname].str.replace('\\n', ' ')\n",
    "    df[colname] = df[colname].str.replace('\\r', ' ')\n",
    "    alphanumeric_characters_extended = '(\\\\b[-/]\\\\b|[a-zA-Z0-9])'\n",
    "    df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), ' ')\n",
    "    return df\n",
    "\n",
    "def tokenize_sentence(df, colname):\n",
    "    df[colname] = df[colname].str.split()\n",
    "    return df\n",
    "\n",
    "def remove_stop_words(df, colname):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['deleted', 'http'])\n",
    "    for idx, val in df[colname].items():\n",
    "        if type(val) == list:\n",
    "            df.at[idx, colname] = [word for word in val if word not in stop_words]\n",
    "        else:\n",
    "            df.at[idx, colname] = [' ']\n",
    "    return df\n",
    "\n",
    "def lemmatize_words(df, colname):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[colname] = df[colname].apply(lambda com: [lemmatizer.lemmatize(word) for word in com])\n",
    "    return df\n",
    "\n",
    "def reverse_tokenize_sentence(df, colname):\n",
    "    df[colname] = df[colname].map(lambda word: ' '.join(word))\n",
    "    return df\n",
    "\n",
    "\n",
    "def text_cleaning(df, colname):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. convert text to lowercase\n",
    "    2. remove punctuation and new line characters '\\n'\n",
    "    3. Tokenize sentences\n",
    "    4. Remove all stopwords\n",
    "    5. convert tokenized text to text\n",
    "    \"\"\"\n",
    "    df = (\n",
    "      df\n",
    "      .pipe(convert_text_to_lowercase, colname)\n",
    "      .pipe(remove_punctuation, colname)\n",
    "      .pipe(tokenize_sentence, colname)\n",
    "      .pipe(remove_stop_words, colname)\n",
    "      .pipe(lemmatize_words, colname)\n",
    "      .pipe(reverse_tokenize_sentence, colname)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P77v7kjMzZ_-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-d9f8bc20f3ec>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), ' ')\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = text_cleaning(new_df, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4234970, 34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0       1676837\n",
       "2.0        575082\n",
       "3.0        219060\n",
       "0.0        153372\n",
       "4.0         75130\n",
       "           ...   \n",
       "2193.0          1\n",
       "2942.0          1\n",
       "3962.0          1\n",
       "4559.0          1\n",
       "3006.0          1\n",
       "Name: ups, Length: 4023, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.ups.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to the TF-IDF, we add a column to the dataset which corresponds to the length of the comment. It is an additional information for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['comment_length'] = df_cleaned['body'].apply(lambda com: len(com) if pd.notnull(com) and\n",
    "                                                        com!='deleted' and com!='[deleted]' \n",
    "                                                        else 0\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-XQl8GN32RD"
   },
   "source": [
    "Save new dataset with `body` cleaned and new column `comment_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vT0nj0eWJIHp"
   },
   "outputs": [],
   "source": [
    "file1=open(repsource+\"df_body_cleaned\",\"wb\")\n",
    "pickle.dump(df_cleaned,file1)\n",
    "file1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
